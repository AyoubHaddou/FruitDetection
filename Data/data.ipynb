{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_compteur_fruit = {}\n",
    "i=0\n",
    "for root, dir, filenames in os.walk('fruits-360_dataset/fruits-360/Training'):\n",
    "    for filename in filenames:\n",
    "        \n",
    "        name = root.split(\"\\\\\")[-1]\n",
    "        n = name.split(\" \")[0]\n",
    "        if n in list(dic_compteur_fruit.keys()):\n",
    "            dic_compteur_fruit[n] += 1\n",
    "        else:\n",
    "            dic_compteur_fruit[n] = 0\n",
    "        old_path = root + \"/\" + filename\n",
    "        newpath = \"/\".join(root.split(\"/\")[0:-1]) + \"/train/\" + n + \"/\" + n + \"_\" + str(dic_compteur_fruit[n])+\".jpg\"\n",
    "        os.rename(old_path,newpath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_compteur_fruit = {}\n",
    "i=0\n",
    "for root, dir, filenames in os.walk('fruits-360_dataset/fruits-360/Test'):\n",
    "    for filename in filenames:\n",
    "        \n",
    "        name = root.split(\"\\\\\")[-1]\n",
    "        n = name.split(\" \")[0]\n",
    "        if n in list(dic_compteur_fruit.keys()):\n",
    "            dic_compteur_fruit[n] += 1\n",
    "        else:\n",
    "            dic_compteur_fruit[n] = 0\n",
    "        old_path = root + \"/\" + filename\n",
    "        newpath = \"/\".join(root.split(\"/\")[0:-1]) + \"/testing/\" + n + \"/\" + n + \"_\" + str(dic_compteur_fruit[n])+\".jpg\"\n",
    "        os.rename(old_path,newpath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Apple': 2133,\n",
       " 'Apricot': 163,\n",
       " 'Avocado': 308,\n",
       " 'Banana': 483,\n",
       " 'Beetroot': 149,\n",
       " 'Blueberry': 153,\n",
       " 'Cactus': 165,\n",
       " 'Cantaloupe': 327,\n",
       " 'Carambula': 165,\n",
       " 'Cauliflower': 233,\n",
       " 'Cherry': 1147,\n",
       " 'Chestnut': 152,\n",
       " 'Clementine': 165,\n",
       " 'Cocos': 165,\n",
       " 'Corn': 303,\n",
       " 'Cucumber': 285,\n",
       " 'Dates': 165,\n",
       " 'Eggplant': 155,\n",
       " 'Fig': 233,\n",
       " 'Ginger': 98,\n",
       " 'Granadilla': 165,\n",
       " 'Grape': 1145,\n",
       " 'Grapefruit': 329,\n",
       " 'Guava': 165,\n",
       " 'Hazelnut': 156,\n",
       " 'Huckleberry': 165,\n",
       " 'Kaki': 165,\n",
       " 'Kiwi': 155,\n",
       " 'Kohlrabi': 156,\n",
       " 'Kumquats': 165,\n",
       " 'Lemon': 329,\n",
       " 'Limes': 165,\n",
       " 'Lychee': 165,\n",
       " 'Mandarine': 165,\n",
       " 'Mango': 307,\n",
       " 'Mangostan': 101,\n",
       " 'Maracuja': 165,\n",
       " 'Melon': 245,\n",
       " 'Mulberry': 163,\n",
       " 'Nectarine': 323,\n",
       " 'Nut': 395,\n",
       " 'Onion': 450,\n",
       " 'Orange': 159,\n",
       " 'Papaya': 163,\n",
       " 'Passion': 165,\n",
       " 'Peach': 573,\n",
       " 'Pear': 1688,\n",
       " 'Pepino': 165,\n",
       " 'Pepper': 825,\n",
       " 'Physalis': 327,\n",
       " 'Pineapple': 328,\n",
       " 'Pitahaya': 165,\n",
       " 'Plum': 596,\n",
       " 'Pomegranate': 163,\n",
       " 'Pomelo': 152,\n",
       " 'Potato': 600,\n",
       " 'Quince': 165,\n",
       " 'Rambutan': 163,\n",
       " 'Raspberry': 165,\n",
       " 'Redcurrant': 163,\n",
       " 'Salak': 161,\n",
       " 'Strawberry': 409,\n",
       " 'Tamarillo': 165,\n",
       " 'Tangelo': 165,\n",
       " 'Tomato': 1706,\n",
       " 'Walnut': 248,\n",
       " 'Watermelon': 156}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_compteur_fruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip=True,\n",
    "    preprocessing_function=tf.keras.applications.resnet50.preprocess_input,\n",
    "    dtype=tf.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "IMG_SIZE = 75\n",
    "TRAIN_DIR = 'fruits-360_dataset/fruits-360/train'\n",
    "TEST_DIR = 'fruits-360_dataset/fruits-360/testing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 67692 images belonging to 67 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batch = train_gen.flow_from_directory(\n",
    "    directory=TRAIN_DIR,\n",
    "    target_size=(IMG_SIZE,IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"sparse\",\n",
    "    seed=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22688 images belonging to 67 classes.\n"
     ]
    }
   ],
   "source": [
    "test_batch = train_gen.flow_from_directory(\n",
    "    directory=TEST_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode = 'sparse',\n",
    "    seed=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 55s 1us/step\n",
      "94781440/94765736 [==============================] - 55s 1us/step\n"
     ]
    }
   ],
   "source": [
    "IMG_SHAPE = (IMG_SIZE,IMG_SIZE) + (3,)\n",
    "base_model = tf.keras.applications.ResNet50(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False\n",
    "inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = tf.keras.layers.Dense(67, activation='softmax')(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "24/24 [==============================] - 38s 2s/step - loss: 0.6279 - accuracy: 0.8279 - val_loss: 0.5995 - val_accuracy: 0.8307\n",
      "Epoch 2/50\n",
      "24/24 [==============================] - 39s 2s/step - loss: 0.5377 - accuracy: 0.8416 - val_loss: 0.5248 - val_accuracy: 0.8581\n",
      "Epoch 3/50\n",
      "24/24 [==============================] - 40s 2s/step - loss: 0.4690 - accuracy: 0.8636 - val_loss: 0.4626 - val_accuracy: 0.8717\n",
      "Epoch 4/50\n",
      "24/24 [==============================] - 38s 2s/step - loss: 0.4130 - accuracy: 0.8818 - val_loss: 0.4433 - val_accuracy: 0.8809\n",
      "Epoch 5/50\n",
      "24/24 [==============================] - 39s 2s/step - loss: 0.3571 - accuracy: 0.8996 - val_loss: 0.3847 - val_accuracy: 0.8978\n",
      "Epoch 6/50\n",
      "24/24 [==============================] - 39s 2s/step - loss: 0.3280 - accuracy: 0.9015 - val_loss: 0.3712 - val_accuracy: 0.8952\n",
      "Epoch 7/50\n",
      "24/24 [==============================] - 38s 2s/step - loss: 0.2820 - accuracy: 0.9233 - val_loss: 0.3412 - val_accuracy: 0.8997\n",
      "Epoch 8/50\n",
      "24/24 [==============================] - 38s 2s/step - loss: 0.2685 - accuracy: 0.9235 - val_loss: 0.3406 - val_accuracy: 0.9023\n",
      "Epoch 9/50\n",
      "24/24 [==============================] - 38s 2s/step - loss: 0.2333 - accuracy: 0.9331 - val_loss: 0.3081 - val_accuracy: 0.9062\n",
      "Epoch 10/50\n",
      "24/24 [==============================] - 38s 2s/step - loss: 0.2383 - accuracy: 0.9320 - val_loss: 0.3135 - val_accuracy: 0.9085\n",
      "Epoch 11/50\n",
      "24/24 [==============================] - 38s 2s/step - loss: 0.2129 - accuracy: 0.9403 - val_loss: 0.2647 - val_accuracy: 0.9212\n",
      "Epoch 12/50\n",
      "24/24 [==============================] - 38s 2s/step - loss: 0.2016 - accuracy: 0.9399 - val_loss: 0.2467 - val_accuracy: 0.9245\n",
      "Epoch 13/50\n",
      "24/24 [==============================] - 38s 2s/step - loss: 0.1948 - accuracy: 0.9443 - val_loss: 0.2707 - val_accuracy: 0.9167\n",
      "Epoch 14/50\n",
      "24/24 [==============================] - 38s 2s/step - loss: 0.1713 - accuracy: 0.9525 - val_loss: 0.2521 - val_accuracy: 0.9209\n"
     ]
    }
   ],
   "source": [
    "h = model.fit(train_batch, \n",
    "            epochs=50,\n",
    "            steps_per_epoch = 6231 // BATCH_SIZE,\n",
    "            validation_data = test_batch,\n",
    "            validation_steps = 3114 // BATCH_SIZE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            callbacks = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a56e509252da22a4a4f93ae404dbc759abc3f6493d8365896b99c773e73ebf8f"
  },
  "kernelspec": {
   "display_name": "Python 3.6.15 ('aifruits')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
